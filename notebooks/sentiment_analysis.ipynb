{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52d6c8f",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f55823d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification, \n",
    "    AdamW,\n",
    "    get_scheduler\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5803dcd",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b127c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and preprocess text data.\"\"\"\n",
    "        text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "        text = text.lower() #convert to lower case\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "        text = re.sub(r'#(\\w+)', r'\\1', text)  # Remove hashtags but keep the word\n",
    "        text = re.sub(r'[^a-z0-9\\s]', ' ', text)  # Keep only alphanumeric\n",
    "        text = ' '.join(word for word in text.split() if word not in self.stop_words)\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6228b",
   "metadata": {},
   "source": [
    "### Dataset for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c790ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, reviews, sentiments, tokenizer, max_length):\n",
    "        self.reviews = reviews\n",
    "        self.sentiments = sentiments\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = str(self.reviews[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.sentiments[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59fac0",
   "metadata": {},
   "source": [
    "### Sentiment Analyzer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9814ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    def __init__(self, model_path=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # uses GPU if available\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.max_length = 512\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        \n",
    "        if model_path:\n",
    "            self.model = self.load_model(model_path)\n",
    "        else:\n",
    "            self.model = None\n",
    "\n",
    "    def prepare_data(self, df, test_size=0.2):\n",
    "        \"\"\"Prepare and split data for training.\"\"\"\n",
    "        # Clean the reviews\n",
    "        df['review'] = df['review'].apply(self.preprocessor.clean_text)\n",
    "        \n",
    "        # Convert sentiments to numerical values\n",
    "        df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "        \n",
    "        # Split the data\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            df['review'].values, \n",
    "            df['sentiment'].values, \n",
    "            test_size=test_size, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = IMDbDataset(train_texts, train_labels, self.tokenizer, self.max_length)\n",
    "        val_dataset = IMDbDataset(val_texts, val_labels, self.tokenizer, self.max_length)\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def train_model(self, trial, train_dataset, val_dataset):\n",
    "        \"\"\"Train model with hyperparameter optimization.\"\"\"\n",
    "        # Hyperparameters\n",
    "        batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True)\n",
    "        num_epochs = 3\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            'distilbert-base-uncased', \n",
    "            num_labels=2\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Optimizer and scheduler\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "        num_training_steps = num_epochs * len(train_loader)\n",
    "        scheduler = get_scheduler(\n",
    "            \"linear\",\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "        \n",
    "        best_val_acc = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            train_preds, train_labels = [], []\n",
    "            \n",
    "            for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                train_preds.extend(preds.cpu().numpy())\n",
    "                train_labels.extend(batch['labels'].cpu().numpy())\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_preds, val_labels = [], []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                    outputs = model(**batch)\n",
    "                    \n",
    "                    preds = torch.argmax(outputs.logits, dim=1)\n",
    "                    val_preds.extend(preds.cpu().numpy())\n",
    "                    val_labels.extend(batch['labels'].cpu().numpy())\n",
    "            \n",
    "            # Calculate metrics\n",
    "            val_acc = accuracy_score(val_labels, val_preds)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                val_labels, \n",
    "                val_preds, \n",
    "                average='binary'\n",
    "            )\n",
    "            \n",
    "            print(f'\\nEpoch {epoch+1} metrics:')\n",
    "            print(f'Validation Accuracy: {val_acc:.4f}')\n",
    "            print(f'Precision: {precision:.4f}')\n",
    "            print(f'Recall: {recall:.4f}')\n",
    "            print(f'F1 Score: {f1:.4f}')\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                self.model = model\n",
    "            \n",
    "            trial.report(val_acc, epoch)\n",
    "            \n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        return best_val_acc\n",
    "\n",
    "    def predict(self, text):\n",
    "        \"\"\"Predict sentiment for a given text.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model hasn't been trained or loaded yet!\")\n",
    "            \n",
    "        # Preprocess the text\n",
    "        cleaned_text = self.preprocessor.clean_text(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            cleaned_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        # Get prediction\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "            probs = F.softmax(outputs.logits, dim=1)\n",
    "            \n",
    "        negative_score, positive_score = probs[0].cpu().numpy()\n",
    "        \n",
    "        return {\n",
    "            'negative': float(negative_score),\n",
    "            'positive': float(positive_score),\n",
    "            'sentiment': 'positive' if positive_score > negative_score else 'negative',\n",
    "            'confidence': float(max(negative_score, positive_score))\n",
    "        }\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save the trained model and tokenizer.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model to save!\")\n",
    "        \n",
    "        self.model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        \n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load a trained model.\"\"\"\n",
    "        return DistilBertForSequenceClassification.from_pretrained(path).to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f3bfe",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7efa280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankus\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "[I 2024-11-20 15:47:49,598] A new study created in memory with name: no-name-67a48029-57e3-46cb-9da5-38fd9a554f09\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ankus\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1e9a3371c646e6aae0203ed5cb5a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 metrics:\n",
      "Validation Accuracy: 0.9146\n",
      "Precision: 0.9331\n",
      "Recall: 0.8946\n",
      "F1 Score: 0.9135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd669c321ee4ec29a7108e1be9b2316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 metrics:\n",
      "Validation Accuracy: 0.9169\n",
      "Precision: 0.9211\n",
      "Recall: 0.9133\n",
      "F1 Score: 0.9172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2d0dbe757545f8b1df4428dbcfb8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 17:04:32,177] Trial 0 finished with value: 0.9169 and parameters: {'batch_size': 16, 'learning_rate': 1.4485703536841932e-05}. Best is trial 0 with value: 0.9169.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 metrics:\n",
      "Validation Accuracy: 0.9151\n",
      "Precision: 0.9162\n",
      "Recall: 0.9153\n",
      "F1 Score: 0.9157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ankus\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8de988a3dd2489d852d93180126fc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 metrics:\n",
      "Validation Accuracy: 0.9100\n",
      "Precision: 0.8892\n",
      "Recall: 0.9383\n",
      "F1 Score: 0.9131\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f83490992b4387b1f5ebcbc45b54e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 metrics:\n",
      "Validation Accuracy: 0.9180\n",
      "Precision: 0.9059\n",
      "Recall: 0.9343\n",
      "F1 Score: 0.9199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206922b750f8465398be9baddacc6e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-22 09:45:43,709] Trial 1 finished with value: 0.918 and parameters: {'batch_size': 8, 'learning_rate': 1.8502624391120073e-05}. Best is trial 1 with value: 0.918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 metrics:\n",
      "Validation Accuracy: 0.9179\n",
      "Precision: 0.9225\n",
      "Recall: 0.9139\n",
      "F1 Score: 0.9182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ankus\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da9724484b148d59e6aa2dcd7b33c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 metrics:\n",
      "Validation Accuracy: 0.9029\n",
      "Precision: 0.9341\n",
      "Recall: 0.8686\n",
      "F1 Score: 0.9002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eef4254369e44f180702a60e6453d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 metrics:\n",
      "Validation Accuracy: 0.9143\n",
      "Precision: 0.9274\n",
      "Recall: 0.9004\n",
      "F1 Score: 0.9137\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df6eb7b15f843ab82250967164005cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 13:11:08,534] Trial 2 finished with value: 0.9149 and parameters: {'batch_size': 16, 'learning_rate': 1.5156692683609128e-05}. Best is trial 1 with value: 0.918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 metrics:\n",
      "Validation Accuracy: 0.9149\n",
      "Precision: 0.9130\n",
      "Recall: 0.9186\n",
      "F1 Score: 0.9158\n",
      "\n",
      "Best trial:\n",
      "Value: 0.9180\n",
      "Params: {'batch_size': 8, 'learning_rate': 1.8502624391120073e-05}\n",
      "\n",
      "Review: This movie was absolutely fantastic! I loved every minute of it.\n",
      "Prediction: {'negative': 0.004245401360094547, 'positive': 0.995754599571228, 'sentiment': 'positive', 'confidence': 0.995754599571228}\n",
      "\n",
      "Review: What a terrible waste of time. I couldn't even finish watching it.\n",
      "Prediction: {'negative': 0.9982640147209167, 'positive': 0.0017359622288495302, 'sentiment': 'negative', 'confidence': 0.9982640147209167}\n",
      "\n",
      "Review: It was okay, nothing special but not bad either.\n",
      "Prediction: {'negative': 0.802784264087677, 'positive': 0.19721579551696777, 'sentiment': 'negative', 'confidence': 0.802784264087677}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize the analyzer\n",
    "    analyzer = SentimentAnalyzer()\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv('IMDB_dataset.csv')\n",
    "    train_dataset, val_dataset = analyzer.prepare_data(df)\n",
    "    \n",
    "    # Optimize hyperparameters\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    objective = lambda trial: analyzer.train_model(trial, train_dataset, val_dataset)\n",
    "    study.optimize(objective, n_trials=3)\n",
    "    \n",
    "    print(f\"\\nBest trial:\")\n",
    "    print(f\"Value: {study.best_trial.value:.4f}\")\n",
    "    print(f\"Params: {study.best_trial.params}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    analyzer.save_model('best_sentiment_model')\n",
    "    \n",
    "    # Test the model\n",
    "    test_reviews = [\n",
    "        \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "        \"What a terrible waste of time. I couldn't even finish watching it.\",\n",
    "        \"It was okay, nothing special but not bad either.\"\n",
    "    ]\n",
    "    \n",
    "    for review in test_reviews:\n",
    "        result = analyzer.predict(review)\n",
    "        print(f\"\\nReview: {review}\")\n",
    "        print(f\"Prediction: {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
